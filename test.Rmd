---
title: "Milestone 7"
author: "Maria Burzillo"
date: "3/28/2020"
output: 
  pdf_document:
    extra_dependencies: ["rotating"]
bibliography: trounstine_bib.bib
#biblio-style: "apalike"
link_citations: true
header_includes:
  \usepackage{float}
  \usepackage{dcolumn}
  \usepackage{rotating}
  \restylefloat{table}
  
---



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.align = "center")
knitr::write_bib(c("knitr", "stringr"), "bib.bib", width = 60)
library(haven)
library(gtable)
library(stargazer)
library(tidyverse)
library(gt)
library(gtsummary)
library(knitr)
library(stargazer)
library(lme4)
library(margins)
library(reprex)
library(plm)
library(devtools)
library(lfe)
library(Statamarkdown)
library(readxl)
library(starpolishr)
library(AER)
library(readxl)
library(naniar)
library(mice)
library(gtsummary)
library(VIM)
```

```{r data_imports, include = FALSE}

# import both datasets

rp <- read_dta("racial_polarization_winners.dta")
fin_seg<- read_dta("fin_seg.dta")

# add row numbers for fin_seg

fin_seg$row_num <- as.integer(rownames(fin_seg))

# import stata geo_id data

regression_ids_edit <- read_excel("regression_ids_edit.xlsx")

nrow(regression_ids_edit %>%
  filter(in_reg == 1))


# find the ones in the stata regression and run it just on those for a sanity check. then try to see why they were included in one but not the other 

```


```{r data_cleaning_rp, include = FALSE}

# create a factor for year for the rp dataset
rp$year.f <- as.factor(rp$year)

# apply the condition that winner == 1 as in the Stata code for the rp regressions

rp_1<- rp %>%
  filter(winner == 1) %>%
  mutate(medincinterp = medincinterp/1000)

```



# Extension Part 2: Fin Seg Data Set

In the second, and arguably most important dataset in the analysis, the data set on financial segregation, there are a very large number of missing values. 

```{r}

# apply the conditions in Stata regression

fin_imp <- fin_seg %>%
  filter(totaltractsf > 1 | is.na(totaltractsf), dgepercap_cpi != 0) %>%
  mutate(medinc_cpi = medinc_cpi/1000)

# select the relevant variables

fin_imp <- fin_imp %>%
  select(H_citytract_NHW_i, dgepercap_cpi, diversityinterp, pctblkpopinterp, pctasianpopinterp,
         pctlatinopopinterp, medinc_cpi, pctlocalgovworker_100, pctrentersinterp, pctover65,
         pctcollegegradinterp, logpop, geo_id2, chng5pctblk, chng5pctlatino, chng5pctasian,
         ideology_fill)


#miss_var_summary(fin_imp)

#gg_miss_var(fin_imp, show_pct = TRUE)

#gg_miss_var(fin_imp)

```

```{r eval=FALSE}

# select all variables with missing values for the aggr plot

fin_imp_aggr <- fin_imp %>%
  select(H_citytract_NHW_i, diversityinterp, pctblkpopinterp, pctasianpopinterp,
         pctlatinopopinterp, medinc_cpi, pctlocalgovworker_100, pctrentersinterp, pctover65,
         pctcollegegradinterp, chng5pctblk, chng5pctlatino, chng5pctasian,
         ideology_fill)

#aggr_plot <- aggr(fin_imp_aggr, bars = TRUE, col=c('navyblue','red'), numbers=TRUE, sortVars=TRUE, cex.axis=.5, gap=3, ylab=c("Histogram of missing data","Pattern"))


# this plot tells us that only .066% of the data are complete (not missing anything)

nrow(fin_imp) * .0058
```

```{r}

# perform multiple imputations. We have the same issue here with the default method as with the previous dataset so we use CART. Additionally, because of the extremely large size of this dataset, we choose to only impute the variable H_citytract_NHW_i, as it is the most frequently missing variable and also out main independent variable. Just imputing this allows us to include way more observations in the regression than the original data despite the still large number of NAs overall. One concern here would be that if there was some reason that this variable was missing rather than other variables that only imputing this variable could lead to some bias in the results.  

# generate a blank run to get the method matrix

ini <- mice(fin_imp, maxit=0, print=F)

# view the current method settings

meth <- ini$method

# change the method variables for all variables except H_citytract_NHW_i to ""
# so that their missing values aren't imputed

meth[c("dgepercap_cpi", "diversityinterp", "pctblkpopinterp", "pctasianpopinterp",
         "pctlatinopopinterp", "medinc_cpi", "pctlocalgovworker_100", "pctrentersinterp",
         "pctover65", "pctcollegegradinterp", "logpop", "geo_id2", "chng5pctblk", 
       "chng5pctlatino", "chng5pctasian", "ideology_fill")]=""

# change the default method to "cart" so that we don't get the error

meth[c("H_citytract_NHW_i")] = "cart"

# perform 5 imputations of the data

imp_1_fs <- mice(fin_imp, method = meth, maxit = 5)

gg_miss_var(complete(imp_1_fs), show_pct = T)


miss_var_summary(complete(imp_1_fs))
miss_var_summary(fin_imp)

# another try

meth <- ini$method

# change the method variables for all variables except H_citytract_NHW_i to ""
# so that their missing values aren't imputed

meth[c("dgepercap_cpi", "diversityinterp", "pctblkpopinterp", "pctasianpopinterp",
         "pctlatinopopinterp", "medinc_cpi", "pctlocalgovworker_100", "pctrentersinterp",
         "pctover65", "pctcollegegradinterp", "logpop", "geo_id2")]=""

# other

other <- c("chng5pctblk", 
       "chng5pctlatino", "chng5pctasian", "ideology_fill")

# change the default method to "cart" so that we don't get the error

meth[c("H_citytract_NHW_i", "chng5pctblk", 
       "chng5pctlatino", "chng5pctasian", "ideology_fill")] = "cart"

#imp_2_fs <- mice(fin_imp, method = meth, maxit = 5)

# now there are manyb fewer missing variables!!!

#miss_var_summary(complete(imp_2_fs))

#head(complete(imp_2_fs))

```


Finding no clear patterns in the missing data, I next performed mutliple imputations (with 5 iterations) on the dataset. A non-stochastic imputation method, Classification and Regression Trees (CART), was used instead of the default because of an error with matrix inversion caused by the data. Before examining the results of Trounstine's model using the imputed data, I first run some diagnostic tests of the imputation results to make sure that everything is running as expected. 

First, I check the convergence of the algorithm used within mice() for each of the variables. For the most part, the fits intertwine and do not exhibit any trends at later iterations, as desired. 

```{r imp_plot, echo = FALSE}

# ideally, the fits will intertwine and not exhibit any trends at later iterations. This seems to generally be the case. There are some issues with the results for some of the variables with very few missing values, which makes sense as we would expect the mean to be less reliable due to the law of large numbers and thus convergence is more difficult. However, since these are so few values, it is less important about how robust they are. 

#plot(imp_1_fs)

```

We can also check the imputed values against the original values using stripplot(). Each column in each subplot represents a separate iteration. The magenta points represent the imputed data. The values of the variable in questions are along the y axis. We expect the spread of the data to be similar if the imputations were done well. If the data were missing completely at random, then the imputed data should have the same distribution as the original data. In particular, we want to be sure that the imputations are within a plausible range of the data. This is the case for all of our imputed variables, and there does not seem to be cause for alarm from these results. 

```{r rp_stripplot, echo = FALSE}

# we can also check the imputed values against the original values using stripplot(). Each column in each subplot represents a separate iteration. The magenta points represent the imputed data. The values of the variable in questions are along the y axis. We expect the spread of the data to be similar if the imputations were done well. If the data were missing completely at random, then the imputed data should have the same distribution as the original data. In particular, we want to be sure that the imputations are within a plausible range of the data. This is the case for all of our imputed variables, and there does not seem to be cause for alarm from these results. 

# this looks good? All reasonable values...

#stripplot(x = imp_1_fs, data = ~H_citytract_NHW_i)


```

Finally, we can also look at the density plots for each variable's actual data and for their imputed data from each of the iterations, which are represented in magenta. Overall, the density plots align quite well for the variables with the most missing data, biracial and biggestsplit, and relatively well for most of the other variables with missing data. Again, the fit is less good for variables with fewer missing data points. 

```{r rp_densityplot, echo = FALSE}

#densityplot(x=imp_1_fs, data= ~ H_citytract_NHW_i)
```

While the imputations are not perfect, there do not seem to be any major problems so far. Thus, we can now proceed with our analysis. We fit each of our ten imputed datasets to Trounstine's 3 models using this data and then pool the results for each. The results are as follows:


```{r}
# Main Analysis 2: Imputations


## regression 1 Table 2

# fit multiple imputed datasets

fit_imp_felm1 <- with(imp_1_fs, felm(dgepercap_cpi ~ H_citytract_NHW_i + diversityinterp + pctblkpopinterp + pctasianpopinterp + pctlatinopopinterp + medinc_cpi + pctlocalgovworker_100 + pctrentersinterp + pctover65 + pctcollegegradinterp + logpop | factor(geo_id2) |0| geo_id2))

# pool the analyses

pool_imp_felm1 <- pool(fit_imp_felm1)
imp_felm1_sum <- summary(pool_imp_felm1)


## regression 2 Table 2

# fit multiple imputed datasets

fit_imp_felm2 <- with(imp_1_fs, felm(dgepercap_cpi ~ H_citytract_NHW_i + pctblkpopinterp + pctasianpopinterp + pctlatinopopinterp + chng5pctblk + chng5pctlatino + chng5pctasian+ medinc_cpi + pctlocalgovworker_100 + pctrentersinterp + pctover65 + pctcollegegradinterp + logpop | factor(geo_id2) |0| geo_id2))

# pool the analyses

pool_imp_felm2 <- pool(fit_imp_felm2)
imp_felm2_sum <- summary(pool_imp_felm2)



## Regression 3 Table 2

fit_imp_felm3 <- with(imp_1_fs, felm(dgepercap_cpi ~ H_citytract_NHW_i + diversityinterp + pctblkpopinterp + pctasianpopinterp + pctlatinopopinterp + medinc_cpi + pctlocalgovworker_100 + pctrentersinterp + pctover65 + pctcollegegradinterp + logpop + ideology_fill | factor(geo_id2) |0| geo_id2))

# pool the analyses

pool_imp_felm3 <- pool(fit_imp_felm3)
imp_felm3_sum <- summary(pool_imp_felm3)

```

## Regression 1

The results of the first regression indicate that Trounstine's results are robust to the inclusion of the imputed data. The coefficients, standard errors, and significance levels are essentially unchanged. The same is largely true for the second and third specifications, but the magnitude of the coefficient on the segregation decreases by a marginal amount. 

```{r results_printing_fs_impute_regressions_1, echo=FALSE}

# print results of 3rd regression

imp_felm1_sum_table <- as.data.frame(imp_felm1_sum %>% 
  select(term, estimate, std.error, p.value))
print(imp_felm1_sum_table)
```

Despite some progress made towards racial equity in the U.S. on other fronts, residential racial segregation continues to be prevasive and deeply entrenched in society [@fischer_distinguishing_2004; @oliver_paradoxes_2010; @massey_american_1993]. Research suggests that this kind of segregation has political consequences, as political cleavages in segregated cities tend to have racial as well as spatial dimensions [@massey_american_1993]. Neighborhoods are often important actors within local politics because local governments provide many functions that are allocational in nature and concern geographical space [@trounstine_segregation_2016]. Thus, when neighborhoods are divided on racial lines as well as spacial lines, it is natural to expect higher degrees of racial polarization as a result. 

Studying residential segregation is difficult because its effects tend to differ by geographic level. On the neighborhood level, the kind of geographic racial isolation brought on by residential segregation has been associated with racial intolerance, resentment, and competition between racial groups [@oliver_paradoxes_2010]. Living within segregated neighborhoods has also been associated with holding negative stereotypes and perceptions about out groups [@eric_oliver_intergroup_2003]. As a result, homogeneous neighborhoods have been associated with increased racial tension and political polarization in comparison to integrated, diverse neighborhoods. However, at the city or metropolitan level, the opposite seems to be true: when considering larger geographic areas, diversity and integration are correlated with racial tension, competition, prejudice, lower levels of cooperation, and lower spending on public goods [@oliver_paradoxes_2010; @baqir_public_1999; @hopkins_diversity_2009]. While these differences in the expected effect of segregation on the geographic level may seem confusing at first, they make sense as they suggest that the most severely segregated areas are those that are diverse overall, but have many homogeneous neighborhoods. Thus, while people of different races co-exist within a highly segregated city, they live separately within their own neighborhoods, which creates an environment ripe for racial tension [@trounstine_segregation_2016]. It is thus not simply the level of diversity or integration that matters for racial harmony and cooperation, but their patterns within a larger geographic framework [@trounstine_segregation_2016; @oliver_paradoxes_2010; @bharathi_public_2018]. 


# TABLE SHOULD BE RIGHT HERE

```{r results_printing_rp_impute_regressions_1, echo=FALSE, results="asis"}

# print results of 1st regression

imp_1_rp_sum_table <- as.data.frame(imp_felm1_sum_table %>% 
  select(term, estimate, std.error, p.value)) %>%
  head(13)

stargazer(imp_1_rp_sum_table,
          header = FALSE,
          title = "\\textbf{Racial Polarization in Segregated Cities: A}",
          float = FALSE,
          table.placement = "H")

```

Political polarization along racial lines may lead to decreased public spending and goods provision because groups may have different preferences, which can make compromise hard, and because groups may preceive a disutility in out-groups receiving public goods expenditure @baqir_public_1999. @einstein_divided_2012 found evidence that racial segregation predicts large political divisions at the metropolitan level and that these divisions can create a lack of willingness to compromise and collaborate on local policy problems. @trounstine_segregation_2016 finds similar results at the city level: that residential racial segregation is associated with both increased political division and decreased public spending. Thus, these authors suggest that it is the combination of homogenous neighborhoods within a much larger, diverse geographic area that leads to increased political polarization and reduced public goods spending in local governments. 

More recently, some scholars have called this hypothesis and its importance into question. For example, @lee_ethnic_2018 finds evidence that larger inequalities within the political system favoring socially powerful groups, not local diversity patterns leading to decreased cooperation, may be a better explanation of failures in public goods provision in diverse areas. Others suggest that additional factors, such as income segregation, may be important confounding factors in public goods provision. @an_its_2018, for example, suggests that the more closely related income inequality is to racial inequality, the less investment is made in public goods, and that this interaciton was a better predictor of public goods spending patterns than measures of diversity. There is, in fact, a variety of evidence suggesting that it is meaningful to consider the effects of income inequaltiy and diversity and segregation jointly [@an_its_2018; @massey_american_1993]. Given the active debate in the literature over the relaitonships between diversity, segregation, public spending, and other factors, it is increasingly important to re-examine previously reported findings as a means of robustness checks. 


# Replication

I was able to successfully replicate all of the main results from Trounstine (2016). All regressions and tables were fully replicated in R. As an example, in table X, I replicate Table 1 from page 713 of @trounstine_segregation_2016, which is also included as Figure 1 for comparison. However, I was unable to successfully replicate Trounstine (2016)â€™s marginal effects analyses and margins plots using R. There does not yet appear to be a built-in R function to calculate marginal or predicted effects or to generate margins plots from the complicated multi-level models employed in the original paper, and creating such a function was outside of the scope of this analysis. Nevertheless, these results were successfully replicated in Stata. 


```{r results_printing_fs_impute_regressions_2, echo=FALSE}

# print results of 3rd regression

imp_felm2_sum_table <- as.data.frame(imp_felm2_sum %>% 
  select(term, estimate, std.error, p.value))
print(imp_felm2_sum_table)
```

```{r results_printing_fs_impute_regressions_3, echo=FALSE}

# print results of 3rd regression

imp_felm3_sum_table <- as.data.frame(imp_felm3_sum %>% 
  select(term, estimate, std.error, p.value))
print(imp_felm3_sum_table)
```

```{r}

# practice tables

```



```{r}
# how many observations were included in those 3 analyses?

# basic filter for table for all regressions

# xtreg dgepercap_cpi H_citytract_NHW_i diversityinterp pctblkpopinterp
# pctasianpopinterp pctlatinopopinterp medinc_cpi pctlocalgovworker_100
# pctrentersinterp pctover65 pctcollegegradinterp logpop if totaltracts>1 &
# dgepercap_cpi~=0,fe vce(cluster geo_id2)

sum(is.na(complete(imp_1_fs)$H_citytract_NHW_i))
sum(is.na(complete(imp_1_fs,1)$H_citytract_NHW_i))
sum(is.na(complete(imp_1_fs,2)$H_citytract_NHW_i))
sum(is.na(complete(imp_1_fs,3)$H_citytract_NHW_i))
sum(is.na(complete(imp_1_fs,4)$H_citytract_NHW_i))
sum(is.na(complete(imp_1_fs,5)$H_citytract_NHW_i))

fin <- complete(imp_1_fs) %>%
  filter(!(is.na(dgepercap_cpi)), !(is.na(H_citytract_NHW_i)), !(is.na(diversityinterp)),
         !(is.na(pctblkpopinterp)), !(is.na(pctasianpopinterp)), !(is.na(pctlatinopopinterp)),
         !(is.na(medinc_cpi)), !(is.na(pctlocalgovworker_100)), !(is.na(pctrentersinterp)),
         !(is.na(pctover65)), !(is.na(pctcollegegradinterp)), !(is.na(logpop)))

# create dge variable used in regression

fin_dge <-fin %>% filter(dgepercap_cpi != 0)

fin_dge_tab <- tibble(
  Variable = "Direct General Expenditure per Capita",
  Obs = nrow(fin_dge),
  Mean = mean(fin_dge$dgepercap_cpi, na.rm = T),
  SD = sd(fin_dge$dgepercap_cpi, na.rm = T),
  Min = min(fin_dge$dgepercap_cpi, na.rm = T),
  Max = max(fin_dge$dgepercap_cpi, na.rm = T)
)

print(fin_dge_tab)

# now we have 73,119 observations compared to 13,742 --> this only included an extra 200 obs!!

nrow(fin_seg)

# number of missing seg indexes before imputation: 282334

# number missing now: 222957
282334 - 222957

73119-13742

# we were able to increase sample size by 59,377

miss_var_summary(complete(imp_2_fs))



```
